Steps to import demo data in HBase

1. Create demo tables in HBase

hbase(main):006:0>  create 'stockDataComposite','price','spread','stats'
hbase(main):007:0>  create 'stockDataSimple','data'   

2. Now run hadoop jobs to export data to HBase tables. To run the jobs, please set HADOOP_HOME and HBASE_HOME

export HBASE_HOME=<path to HBase>
export HADOOP_HOME=<path to hadoop>

a. Add all jars in $HBASE_HOME/lib to HADOOP_CLASSPATH. Add following line to $HADOOP_HOME/bin/hadoop

# add Hbase libs to CLASSPATH
for f in $HBASE_HOME/lib/*.jar; do
  CLASSPATH=${CLASSPATH}:$f;
done

Note:- These lines will add hbase dependencies to your hadoop classpath. When your hbase is populated, you can comment these lines so that your hadoop classpath is not altered.

b. Now copy exportComposite and exportSimple folders(kept parallel to this readme) to hdfs and run the jobs 
$HADOOP_HOME/bin/hadoop fs -put exportSimple /
$HADOOP_HOME/bin/hadoop fs -put exportComposite /

$HADOOP_HOME/bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar import stockDataSimple /exportSimple
$HADOOP_HOME/bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar import stockDataComposite /exportComposite

Above process will load demo data in HBase.


If you want to understand how the above data was created, please read on. The following section provides details on the
scripts used to create the above data. It can also be helpful if you want to load more data or work with a different date range.

To download and populate stock data.

1. Fetch stock BSE data from the BSE website for a particular date range by executing createTableData.py.

nube@nube-desktop:~/crux/BseStock$ ./createTableData.py

The above script runs with default arguments and downloads a prespecified stock list in given date range. 

If you need, you can specify arguments like in example done below. 

 Arguments required are 
a. filePath to a file which has the list of stockIds. Data for these stocks will be downloaded. Sample is stockIdsList.txt
b. startDate in MM/dd/yyyy format
c. endDate in MM/dd/yyyy format
d. outputPath where these files will be saved after download.

nube@nube-desktop:~/crux/BseStock$ ./createTableData.py ./stockIdsList.txt 01/06/2011 05/06/2011 output

The above command will fetch files for all stockIds listed in stockIdsList.txt kept parallel to script in BseStock folder. 
The files will be copied to output directory. Each stock's data will be saved with the name of the stockIds. 
Then script concatenates stockId with date(yyyyMMdd), which can be used later as the rowkey while loading.

2. We now want to save this data in HBase. Let us create a table in HBase stockData with column families price, spread and stats

On the hbase shell, this can be done as follows:
create 'stockDataSimple','data'

3. Now that the data is downloaded and prepared, we are ready to import it into HBase.

please set HADOOP_HOME and HBASE_HOME

export HBASE_HOME=<path to HBase>
export HADOOP_HOME=<path to hadoop>

a. Add all jars in $HBASE_HOME/lib to HADOOP_CLASSPATH. Add following line to $HADOOP_HOME/bin/hadoop

# add Hbase libs to CLASSPATH
for f in $HBASE_HOME/lib/*.jar; do
  CLASSPATH=${CLASSPATH}:$f;
done

Note:- These lines will add hbase dependencies to your hadoop classpath. When your hbase is populated, you can comment these lines so that your hadoop classpath is not altered.

b. Load data in hbase:

To put data as strings, 

- Run importtsv command through hadoop to prepare data for bulk load in HBase

$HADOOP_HOME/bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,data:openPrice,data:highPrice,data:lowPrice,data:closePrice,data:wap,data:numShares,data:numTrades,data:turnOver,data:highLow,data:closeOpen -Dimporttsv.separator=","  -Dimporttsv.bulk.output=outputHBase stockDataSimple output

-Dimporttsv.columns is format of table.
-Dimporttsv.separator is to define separator in dataFile
-Dimporttsv.bulk.output is to define outputPath for this job
stockDataSimple is tablename for which we are preparing data
output is the full path to the directory where files generated by createTableData.py are kept

- Run completebulkload command through hadoop to load data in HBase
$HADOOP_HOME/bin/hadoop jar $HBASE_HOME/hbase-0.90.3.jar completebulkload /user/nube/outputHBase stockDataSimple

/user/nube/outputHBase is output of importtsv
stockData is tablename where we want to insert data.
Note: In this method all data is inserted as String type.

OR

To put data as composite rowKey stockId(as dataType string), date(as dataType long) both concatenated. 
In this step all data inserted in table stockDataComposite is Float type except rowkey as composite key, 
numShares and numTrades as long types
 
a. Create table in HBase. On the shell,

create 'stockDataComposite','price','spread','stats'

b. Execute java program PopulateBseData.java, first compile by adding hbase-0.90.3.jar, hadoop-core-0.20.2.jar to classpath of java.
 
nube@nube-desktop:~/project/crux/BseStock$ export CLASSPATH=$CLASSPATH:../hadoop-0.20.2/hadoop-0.20.2-core.jar:../hbase-0.90.3/hbase-0.90.3.jar
nube@nube-desktop:~/project/crux/BseStock$ javac PopulateBseData.java
 
To run, you need to add more jars to java classpath - commons-logging-1.1.1.jar, zookeeper-3.3.2.jar and log4j-1.2.16.jar
This code takes one argument - the location where the downloaded data from the python script is kept.

nube@nube-desktop:~/project/crux/BseStock$ java PopulateBseData ./output 

output is the path to the directory where files generated by createTableData.py is kept.
